{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b25301a1dbaf4cacad9302847625c4e8",
            "05373e2fb8a144c9bcd39af39fd37ff8",
            "6a6523a90ebb4f16ad6f9015e64ea1b6",
            "89a8f43499634aec94cf78b0003bdaa3",
            "cd914a8e4c534bda8e8d5ca77e4a1e44",
            "e2125aa3da9e4e32af5bdb4918d4279f",
            "3521406feee5402db5aae22ba1d87c01",
            "8fb49fd015524aeb96f3bffeb09f51b5",
            "084033dae9e84869b4f3899a5990312a",
            "e11b13edf32a4ac5b8aa3ed2d16d8ffc",
            "7501d1614a7641e691499298fb7171c7",
            "0b5c50faebf5467194f56af22929fe00",
            "b8dd3732de6146c28d02c589d5dc2421",
            "9c24a7be7a784aad91501bc3f28f181d",
            "a9bb08cb1c6342a393e51399821bfeb2",
            "7f4e189dac7d435fbe65867c4f388b6f",
            "16e87c793dae4d4bbae45c1b6867a1d9",
            "695d94f88e2a4fdcb264fa51ec3f8453",
            "004a31fc33ba4fc58c291f51ea281f86",
            "49d6acae7e474073a4318a8abf15e3ab",
            "c0580e7270a748cb8ecff48287ad2558",
            "1a7f381b08f649e4a37b9b88717ce205",
            "4928b3e0c86240ff997dfbd771790ce2",
            "33ac3f2e65844a4f9c05861309c03b94",
            "ecd1d543e95b4acc91bf901e442b60bb",
            "e5c2d40be2c946dfb3eccc4140c0ba55",
            "285c400363f046169e0c609958ad5de9",
            "e4e7b924b7ee4337a8a742a34bb8cba3",
            "e1ea414b1605471590c9341dae033833",
            "5ef2bd50a3e342a386c2968b526a6fe0",
            "161549096bea4f24abf3b0862f583411",
            "3710a72863e844af9a6e1498968116fd",
            "40448f837d484654948f0302ec4b4d47",
            "55bb80b49aad40908cbb5afba2419cfd",
            "eb015f8ddc49425384c2eb8d99ba2c57",
            "5e75389c767e415fb11ecca622697364",
            "2411e66529f540989c2035dfa2dfe96d",
            "6075c6ed5c794439a23371ff2016c858",
            "5d59fda2707f4f0c9872665782a5cf49",
            "d55ddcb8302641b0add2497f519083e9",
            "349b8379dd824eb9bc2eebed950a3d38",
            "4f187fad00ac4d1eae11fa351e48de79",
            "5e8f8d1a8da0485483f4de91e4703cf4",
            "dd158bb6c0574e5cae8d1acf5e1785ad",
            "12e43f1ce98f477d86c1a911c5a61ab2",
            "7db69922599a45a5a9d37975d0898d8a",
            "4bd38f355f6442309726dc52c341abcd",
            "e1bf9472435a4e9d83112330d0743361",
            "8b5d07cde83242f3b5dac93afbfcb99a",
            "3adee483e38a456a9d18673229ca0e0f",
            "e845d13e8aba467cbcdb0fad196aa680",
            "75fe1ed4d9574e64bd0b15f297e09f1e",
            "40ae97058f4b4fa2a5a2c64f3c15b96c",
            "5d8a4c5fb7d741e08ef52c8536e938c3",
            "b3564f28e7724d989177b4cedeb612ab",
            "952b735d132e4b3296780891c4e75769",
            "62cb8bb9fb0e469d9adb225945ec4757",
            "41f953c31c58404bbda4701b0f5d89df",
            "849e91ae256f43a7b2b80616a1d9b49c",
            "2db387119f134b7285373e84bfc3774f",
            "ab4c8493fa2e4c97bde2e355a5442494",
            "56da9c98902c4a8ba900270306bf5fa9",
            "f8ab7f14620b4114994634355a1b4de0",
            "5622b072a53341b68ed88f2f22bb4566",
            "90daf990d8c342968ef7b2c771e19a50",
            "74dbc353b22c4aeeb6a62373ba5843a2",
            "740c39cc5eeb4a9da69cb580b3faaeca",
            "098fbd07420b40bc974fbc0d900a8ead",
            "b29110c359854918b3fc4f7b4e0a8a55",
            "dd0a42b2e31148d5b4f3df0774d73edb",
            "7afd8f31394a401586eb495f4afcfe30",
            "6a9115fd54e74536a3f799599ba3b25b",
            "bf500faa0dc14a0d9c465a916fafb83c",
            "7271d292b9e94744aa487c5f63ea7b35",
            "11dcadd8ffcc488f9b26c0eda4696cef",
            "ad1cee0f58044041b96e12ec744c30e5",
            "fc8473ac7cc14f63bc2e8cb32887bef1",
            "6e86bf5d49ab4350ae95cb5efd210af5",
            "3f01e2d32f6b4557845d7f348852b057",
            "08acb53e9451438e8547950dd6489e03",
            "513b46db5102441196cff8ea8ea3b335",
            "0d14bf7772194b0a9779ef19cc0873ba",
            "991d4f92850844cd9e0dc622083a5cc5",
            "70bb166678724a5eb9a7772201a8154b",
            "228adeb1449b4bc9af9c1b5dd4e6be09",
            "7b6bf12e48904ef4af988fb4094ce245",
            "875552eaf05a4339a048af7915a72679",
            "4cd13609ed9d4e6cb9f9e38641ed8030",
            "2c2c6028f8914347adb2b06c7e79dba0",
            "3e1ee4cb714d45dea6e5ac616803cd75",
            "6f7d41ae8b1647e98466e64a21d9ed05",
            "bf9cdac1394f49ff92c32e4531e0bcd0",
            "b09ef4ff3be845bba1cc9595fa301d97",
            "e751ad777c8249809eecc6110072ef1b",
            "d4c146695cb845ee87f83d29a3f0ec3a",
            "095af5cf6230457185c25767dcccddf7",
            "302c61d7bacf4d2c9a9974751867f79a",
            "c7669a255e8749bd986a06bcb4bc0532",
            "50abbfbf9fd4443f86e2a534dbb9feab"
          ]
        },
        "id": "3pgZrykoqlvp",
        "outputId": "2712243b-e82d-4ce1-eaca-2df62ba0fed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2026.1.15)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cpu)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b25301a1dbaf4cacad9302847625c4e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b5c50faebf5467194f56af22929fe00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4928b3e0c86240ff997dfbd771790ce2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55bb80b49aad40908cbb5afba2419cfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12e43f1ce98f477d86c1a911c5a61ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "952b735d132e4b3296780891c4e75769",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "740c39cc5eeb4a9da69cb580b3faaeca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e86bf5d49ab4350ae95cb5efd210af5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tied weights mapping and config for this model specifies to tie transformer.word_embeddings.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c2c6028f8914347adb2b06c7e79dba0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'temperature', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 31 tasks for evaluation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/31 [00:00<?, ?it/s]Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  3%|▎         | 1/31 [01:02<31:25, 62.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  6%|▋         | 2/31 [01:51<26:26, 54.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 10%|▉         | 3/31 [02:53<27:06, 58.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 13%|█▎        | 4/31 [03:46<25:07, 55.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 16%|█▌        | 5/31 [04:26<21:46, 50.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 19%|█▉        | 6/31 [05:29<22:42, 54.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 23%|██▎       | 7/31 [06:37<23:36, 59.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 26%|██▌       | 8/31 [07:38<22:51, 59.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 29%|██▉       | 9/31 [08:23<20:13, 55.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 32%|███▏      | 10/31 [09:10<18:19, 52.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 35%|███▌      | 11/31 [10:07<17:59, 53.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 39%|███▊      | 12/31 [12:05<23:15, 73.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 42%|████▏     | 13/31 [15:03<31:30, 105.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 45%|████▌     | 14/31 [15:41<24:01, 84.77s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 48%|████▊     | 15/31 [16:36<20:13, 75.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 52%|█████▏    | 16/31 [18:59<23:58, 95.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 55%|█████▍    | 17/31 [19:50<19:13, 82.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 58%|█████▊    | 18/31 [20:32<15:13, 70.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 61%|██████▏   | 19/31 [21:08<12:01, 60.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 65%|██████▍   | 20/31 [26:11<24:22, 132.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 68%|██████▊   | 21/31 [28:44<23:11, 139.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 71%|███████   | 22/31 [29:50<17:33, 117.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 74%|███████▍  | 23/31 [30:58<13:38, 102.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 77%|███████▋  | 24/31 [31:41<09:51, 84.52s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 81%|████████  | 25/31 [32:02<06:32, 65.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 84%|████████▍ | 26/31 [32:36<04:40, 56.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 87%|████████▋ | 27/31 [33:26<03:36, 54.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 90%|█████████ | 28/31 [35:19<03:35, 71.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 94%|█████████▎| 29/31 [35:42<01:54, 57.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 97%|█████████▋| 30/31 [36:26<00:53, 53.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "100%|██████████| 31/31 [37:24<00:00, 72.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved results to /content/hf_results.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_514d0e77-4822-4b56-9569-cdda99edcb75\", \"hf_results.jsonl\", 25732)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -----------------------------------\n",
        "# Install Required Packages\n",
        "# -----------------------------------\n",
        "!pip install transformers tqdm pandas sentencepiece accelerate\n",
        "\n",
        "# -----------------------------------\n",
        "# IMPORTS\n",
        "# -----------------------------------\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# -----------------------------------\n",
        "# CONFIG: INPUT / OUTPUT\n",
        "# -----------------------------------\n",
        "from google.colab import files\n",
        "\n",
        "DATA_PATH = Path(\"/content/FAQ_v1.jsonl\")\n",
        "RESULTS_PATH = Path(\"/content/hf_results.jsonl\")\n",
        "\n",
        "# -----------------------------------\n",
        "# LOAD OPEN HF MODEL (no gated access needed)\n",
        "# -----------------------------------\n",
        "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"  # open, no login required\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Disable tie warning\n",
        "model.config.tie_word_embeddings = False\n",
        "\n",
        "# Use proper generation settings\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,            # GPU, -1 for CPU\n",
        "    max_new_tokens=500,  # Only this, remove max_length\n",
        "    do_sample=True,      # Use True for randomness, False for greedy\n",
        "    temperature=0.7      # Must be >0 if do_sample=True\n",
        ")\n",
        "\n",
        "# -----------------------------------\n",
        "# HELPER FUNCTIONS\n",
        "# -----------------------------------\n",
        "def load_jsonl(file_path):\n",
        "    tasks = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            tasks.append(json.loads(line.strip()))\n",
        "    return tasks\n",
        "\n",
        "def save_jsonl(file_path, data):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def query_hf(prompt, context=\"\"):\n",
        "    full_prompt = f\"Context: {context}\\n\\nQuestion: {prompt}\"\n",
        "    try:\n",
        "        outputs = generator(full_prompt, max_new_tokens=500)\n",
        "        return outputs[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        print(\"Error querying HF model:\", e)\n",
        "        return \"ERROR\"\n",
        "\n",
        "# -----------------------------------\n",
        "# MAIN EVAL LOOP\n",
        "# -----------------------------------\n",
        "def run_evaluation(tasks):\n",
        "    results = []\n",
        "    for task in tqdm(tasks):\n",
        "        response = query_hf(task['prompt'], task.get('context', \"\"))\n",
        "        result_entry = {\n",
        "            \"task_id\": task[\"task_id\"],\n",
        "            \"task_category\": task[\"task_category\"],\n",
        "            \"prompt\": task[\"prompt\"],\n",
        "            \"expected_answer\": task[\"expected_answer\"],\n",
        "            \"model_response\": response,\n",
        "            \"failure_modes_expected\": task.get(\"failure_modes_expected\", []),\n",
        "            \"grounding_required\": task.get(\"grounding_required\", False)\n",
        "        }\n",
        "        results.append(result_entry)\n",
        "    return results\n",
        "\n",
        "# -----------------------------------\n",
        "# RUN EVALUATION\n",
        "# -----------------------------------\n",
        "tasks = load_jsonl(DATA_PATH)\n",
        "print(f\"Loaded {len(tasks)} tasks for evaluation.\")\n",
        "\n",
        "results = run_evaluation(tasks)\n",
        "save_jsonl(RESULTS_PATH, results)\n",
        "print(f\"Saved results to {RESULTS_PATH}\")\n",
        "\n",
        "# ----- Download results -----\n",
        "files.download(str(RESULTS_PATH))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
    
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
